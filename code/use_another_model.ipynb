{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "from zlib import crc32\n",
    "import time\n",
    "import copy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = './input/data/train/images'\n",
    "base_dir = './each_classes'\n",
    "mask_classes = [\"{0:02d}\".format(a) for a in range(3)]\n",
    "gender_classes = [\"{0:02d}\".format(a) for a in range(2)]\n",
    "age_classes = [\"{0:02d}\".format(a) for a in range(3)]\n",
    "\n",
    "if not os.path.isdir(base_dir): \n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "mask_dir = os.path.join(base_dir, 'mask')\n",
    "if not os.path.isdir(mask_dir): \n",
    "    os.makedirs(mask_dir)\n",
    "mask_train_dir = os.path.join(mask_dir, 'train')\n",
    "if not os.path.isdir(mask_train_dir):\n",
    "    os.makedirs(mask_train_dir)\n",
    "mask_test_dir = os.path.join(mask_dir, 'test')\n",
    "if not os.path.isdir(mask_test_dir):\n",
    "    os.makedirs(mask_test_dir)\n",
    "\n",
    "gender_dir = os.path.join(base_dir, 'gender')\n",
    "if not os.path.isdir(gender_dir):\n",
    "    os.makedirs(gender_dir)\n",
    "gender_train_dir = os.path.join(gender_dir, 'train')\n",
    "if not os.path.isdir(gender_train_dir):\n",
    "    os.makedirs(gender_train_dir)\n",
    "gender_test_dir = os.path.join(gender_dir, 'test')\n",
    "if not os.path.isdir(gender_test_dir):\n",
    "    os.makedirs(gender_test_dir)\n",
    "\n",
    "age_dir = os.path.join(base_dir, \"age\")\n",
    "if not os.path.isdir(age_dir): \n",
    "    os.makedirs(age_dir)\n",
    "age_train_dir = os.path.join(age_dir, 'train')\n",
    "if not os.path.isdir(age_train_dir):\n",
    "    os.makedirs(age_train_dir)\n",
    "age_test_dir = os.path.join(age_dir, 'test')\n",
    "if not os.path.isdir(age_test_dir):\n",
    "    os.makedirs(age_test_dir)\n",
    "\n",
    "## mask\n",
    "for classes in mask_classes:\n",
    "    train_split = os.path.join(mask_train_dir, str(classes))\n",
    "    if not os.path.isdir(train_split):\n",
    "        os.mkdir(train_split)\n",
    "    test_split = os.path.join(mask_test_dir, str(classes))\n",
    "    if not os.path.isdir(test_split):\n",
    "        os.mkdir(test_split)\n",
    "\n",
    "# gender\n",
    "for classes in gender_classes:\n",
    "    train_split = os.path.join(gender_train_dir, str(classes))\n",
    "    if not os.path.isdir(train_split):\n",
    "        os.mkdir(train_split)\n",
    "    test_split = os.path.join(gender_test_dir, str(classes))\n",
    "    if not os.path.isdir(test_split):\n",
    "        os.mkdir(test_split)\n",
    "    \n",
    "# age\n",
    "for classes in age_classes:\n",
    "    train_split = os.path.join(age_train_dir, str(classes))\n",
    "    if not os.path.isdir(train_split):\n",
    "        os.mkdir(train_split)\n",
    "    test_split = os.path.join(age_test_dir, str(classes))\n",
    "    if not os.path.isdir(test_split):\n",
    "        os.mkdir(test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_mask_this(data, file):\n",
    "    # 마스크 여부 분류\n",
    "    if 'incorrect' in file:\n",
    "        return 1\n",
    "    elif 'mask' in file:\n",
    "        return 0\n",
    "    else: # not wear\n",
    "        return 2\n",
    "\n",
    "def what_gender_this(data, file):\n",
    "    # 성별 분류\n",
    "    if data['gender'] == 'male':\n",
    "        return 0\n",
    "    else: # female\n",
    "        return 1\n",
    "\n",
    "def what_age_this(data, file):\n",
    "    # 나이 분류\n",
    "    if data['age'] < 30:\n",
    "        return 0\n",
    "    elif data['age'] >= 60:\n",
    "        return 2\n",
    "    else: # 30 ~ 60\n",
    "        return 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_splitter(identifier, test_ratio):\n",
    "    return crc32(np.string_(identifier)) & 0xffffffff < test_ratio * 2 ** 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_infos = pd.read_csv(\"./input/data/train/train.csv\")\n",
    "train_counter = {}\n",
    "test_counter = {}\n",
    "\n",
    "for i in range(3):\n",
    "    train_counter[str(i)] = 0\n",
    "    test_counter[str(i)] = 0\n",
    "\n",
    "train_dir = os.path.join(mask_dir, 'train')\n",
    "test_dir = os.path.join(mask_dir, 'test')\n",
    "\n",
    "for row in data_infos.iterrows():\n",
    "    data_num, data = row[0], row[1]\n",
    "\n",
    "    # 목표 디렉토리 설정\n",
    "    path = os.path.join(original_dataset_dir, data['path'])\n",
    "    # 디렉토리 리스트 가져오기\n",
    "    fnames = os.listdir(path)\n",
    "    # 데이터 분활하기\n",
    "    test_set_check = ratio_splitter(path, 0.15)\n",
    "    \n",
    "    \n",
    "    for file in fnames:\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "\n",
    "        target_class = what_mask_this(data, file)\n",
    "        target_path = \"{0:02d}\".format(target_class)\n",
    "        \n",
    "        src = os.path.join(path, file)\n",
    "\n",
    "        if test_set_check:\n",
    "            dst = os.path.join(os.path.join(test_dir, target_path), file)\n",
    "        else: # train_set\n",
    "            dst = os.path.join(os.path.join(train_dir, target_path), file)\n",
    "\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "        # 파일 확장자를 추출하기 위해서\n",
    "        name, extension = file.split(\".\") \n",
    "\n",
    "        if test_set_check:\n",
    "            change_name = os.path.join(os.path.join(test_dir, target_path), str(test_counter[str(target_class)]) + \".\" + extension)\n",
    "            test_counter[str(target_class)] += 1 \n",
    "        else: # train_set\n",
    "            change_name = os.path.join(os.path.join(train_dir, target_path), str(train_counter[str(target_class)]) + \".\" + extension)\n",
    "            train_counter[str(target_class)] += 1 \n",
    "            \n",
    "        shutil.move(dst, change_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_infos = pd.read_csv(\"./input/data/train/train.csv\")\n",
    "train_counter = {}\n",
    "test_counter = {}\n",
    "\n",
    "for i in range(2):\n",
    "    train_counter[str(i)] = 0\n",
    "    test_counter[str(i)] = 0\n",
    "\n",
    "train_dir = os.path.join(gender_dir, 'train')\n",
    "test_dir = os.path.join(gender_dir, 'test')\n",
    "\n",
    "for row in data_infos.iterrows():\n",
    "    data_num, data = row[0], row[1]\n",
    "\n",
    "    # 목표 디렉토리 설정\n",
    "    path = os.path.join(original_dataset_dir, data['path'])\n",
    "    # 디렉토리 리스트 가져오기\n",
    "    fnames = os.listdir(path)\n",
    "    # 데이터 분활하기\n",
    "    test_set_check = ratio_splitter(path, 0.15)\n",
    "    \n",
    "    \n",
    "    for file in fnames:\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "\n",
    "        target_class = what_gender_this(data, file)\n",
    "        target_path = \"{0:02d}\".format(target_class)\n",
    "        \n",
    "        src = os.path.join(path, file)\n",
    "\n",
    "        if test_set_check:\n",
    "            dst = os.path.join(os.path.join(test_dir, target_path), file)\n",
    "        else: # train_set\n",
    "            dst = os.path.join(os.path.join(train_dir, target_path), file)\n",
    "\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "        # 파일 확장자를 추출하기 위해서\n",
    "        name, extension = file.split(\".\") \n",
    "\n",
    "        if test_set_check:\n",
    "            change_name = os.path.join(os.path.join(test_dir, target_path), str(test_counter[str(target_class)]) + \".\" + extension)\n",
    "            test_counter[str(target_class)] += 1 \n",
    "        else: # train_set\n",
    "            change_name = os.path.join(os.path.join(train_dir, target_path), str(train_counter[str(target_class)]) + \".\" + extension)\n",
    "            train_counter[str(target_class)] += 1 \n",
    "            \n",
    "        shutil.move(dst, change_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_infos = pd.read_csv(\"./input/data/train/train.csv\")\n",
    "train_counter = {}\n",
    "test_counter = {}\n",
    "\n",
    "for i in range(3):\n",
    "    train_counter[str(i)] = 0\n",
    "    test_counter[str(i)] = 0\n",
    "\n",
    "train_dir = os.path.join(age_dir, 'train')\n",
    "test_dir = os.path.join(age_dir, 'test')\n",
    "\n",
    "for row in data_infos.iterrows():\n",
    "    data_num, data = row[0], row[1]\n",
    "\n",
    "    # 목표 디렉토리 설정\n",
    "    path = os.path.join(original_dataset_dir, data['path'])\n",
    "    # 디렉토리 리스트 가져오기\n",
    "    fnames = os.listdir(path)\n",
    "    # 데이터 분활하기\n",
    "    test_set_check = ratio_splitter(path, 0.15)\n",
    "    \n",
    "    \n",
    "    for file in fnames:\n",
    "        if file[0] == '.':\n",
    "            continue\n",
    "\n",
    "        target_class = what_age_this(data, file)\n",
    "        target_path = \"{0:02d}\".format(target_class)\n",
    "        \n",
    "        src = os.path.join(path, file)\n",
    "\n",
    "        if test_set_check:\n",
    "            dst = os.path.join(os.path.join(test_dir, target_path), file)\n",
    "        else: # train_set\n",
    "            dst = os.path.join(os.path.join(train_dir, target_path), file)\n",
    "\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "        # 파일 확장자를 추출하기 위해서\n",
    "        name, extension = file.split(\".\") \n",
    "\n",
    "        if test_set_check:\n",
    "            change_name = os.path.join(os.path.join(test_dir, target_path), str(test_counter[str(target_class)]) + \".\" + extension)\n",
    "            test_counter[str(target_class)] += 1 \n",
    "        else: # train_set\n",
    "            change_name = os.path.join(os.path.join(train_dir, target_path), str(train_counter[str(target_class)]) + \".\" + extension)\n",
    "            train_counter[str(target_class)] += 1 \n",
    "            \n",
    "        shutil.move(dst, change_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "class Color:\n",
    "\n",
    "    HueSaturationValue_isChecked = False\n",
    "    RGBShift_isChecked = True\n",
    "\n",
    "    #입력 이미지의 색조, 채도 및 값을 임의로 변경합니다.\n",
    "    def HueSaturationValue (image, hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=1):\n",
    "        \"\"\"\n",
    "       hue_shift_limit\t[int, int] or int\t\n",
    "        색조 변경 범위. hue_shift_limit가 단일 int이면 범위는 (-hue_shift_limit, hue_shift_limit)입니다. 기본값 : (-20, 20).\n",
    "\n",
    "        sat_shift_limit\t[int, int] or int\t\n",
    "        채도 변경 범위. sat_shift_limit가 단일 정수이면 범위는 (-sat_shift_limit, sat_shift_limit)입니다. 기본값 : (-30, 30).\n",
    "\n",
    "        val_shift_limit\t[int, int] or int\t\n",
    "        값 변경 범위. val_shift_limit가 단일 정수이면 범위는 (-val_shift_limit, val_shift_limit)입니다. 기본값 : (-20, 20).\n",
    "\n",
    "        p\tfloat\t\n",
    "        변환을 적용 할 확률. 기본값 : 1\n",
    "        \"\"\"\n",
    "        transform = A.Compose([\n",
    "            A.HueSaturationValue(hue_shift_limit, sat_shift_limit, val_shift_limit, always_apply, p)\n",
    "        ],p=1)\n",
    "        img = transform(image=image)['image']\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_save(directory, image_data, image_name, excution):\n",
    "    cv2.imwrite(os.path.join(directory, str(image_name) + \".\" + excution), image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sixty_list = [\"02\"]\n",
    "not_mask_list = [\"01\", \"02\"]\n",
    "mans_list = [\"00\"]\n",
    "\n",
    "over_sixty_ratio = 6\n",
    "not_mask_ratio = 5\n",
    "mans_ratio = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in over_sixty_list:\n",
    "    target_class = int(dir)\n",
    "    train_target = os.path.join(age_train_dir, dir)\n",
    "    test_target = os.path.join(age_test_dir, dir)\n",
    "\n",
    "    # 디렉토리 리스트 가져오기\n",
    "    train_fnames = os.listdir(train_target)\n",
    "    test_fnames = os.listdir(test_target)\n",
    "    \n",
    "    for train_file in train_fnames:\n",
    "        for _ in range(over_sixty_ratio):           \n",
    "            aug_img = cv2.imread(os.path.join(train_target, train_file))\n",
    "            aug_img = Color.HueSaturationValue(aug_img)\n",
    "            _, extension = train_file.split(\".\") \n",
    "            image_save(train_target, aug_img, train_counter[str(target_class)], extension)\n",
    "            train_counter[str(target_class)] += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in not_mask_list:\n",
    "    target_class = int(dir)\n",
    "    train_target = os.path.join(mask_train_dir, dir)\n",
    "    test_target = os.path.join(mask_test_dir, dir)\n",
    "\n",
    "    # 디렉토리 리스트 가져오기\n",
    "    train_fnames = os.listdir(train_target)\n",
    "    test_fnames = os.listdir(test_target)\n",
    "    \n",
    "    for train_file in train_fnames:\n",
    "        for _ in range(not_mask_ratio):           \n",
    "            aug_img = cv2.imread(os.path.join(train_target, train_file))\n",
    "            aug_img = Color.HueSaturationValue(aug_img)\n",
    "            _, extension = train_file.split(\".\") \n",
    "            image_save(train_target, aug_img, train_counter[str(target_class)], extension)\n",
    "            train_counter[str(target_class)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in mans_list:\n",
    "    target_class = int(dir)\n",
    "    train_target = os.path.join(gender_train_dir, dir)\n",
    "    test_target = os.path.join(gender_test_dir, dir)\n",
    "\n",
    "    # 디렉토리 리스트 가져오기\n",
    "    train_fnames = os.listdir(train_target)\n",
    "    test_fnames = os.listdir(test_target)\n",
    "    \n",
    "    for train_file in train_fnames:\n",
    "        target_image_path = os.path.join(train_target, train_file)\n",
    "        if ratio_splitter(target_image_path, mans_ratio):      \n",
    "            aug_img = cv2.imread(target_image_path)\n",
    "            aug_img = Color.HueSaturationValue(aug_img)\n",
    "            _, extension = train_file.split(\".\") \n",
    "            image_save(train_target, aug_img, train_counter[str(target_class)], extension)\n",
    "            train_counter[str(target_class)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "print(DEVICE)\n",
    "BATCH_SIZE = 100\n",
    "EPOCH = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.Resize([256, 172]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([transforms.Resize([256, 172]),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\n",
    "data_dir = './each_classes/mask' \n",
    "mask_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'test']} \n",
    "mask_dataloader = {x: torch.utils.data.DataLoader(mask_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'test']} \n",
    "mask_dataset_sizes = {x: len(mask_datasets[x]) for x in ['train', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "data_dir = './each_classes/gender' \n",
    "gender_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'test']} \n",
    "gender_dataloader = {x: torch.utils.data.DataLoader(gender_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'test']} \n",
    "gender_dataset_sizes = {x: len(gender_datasets[x]) for x in ['train', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age\n",
    "data_dir = './each_classes/age' \n",
    "age_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'test']} \n",
    "age_dataloader = {x: torch.utils.data.DataLoader(age_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'test']} \n",
    "age_dataset_sizes = {x: len(age_datasets[x]) for x in ['train', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': 0, '01': 1, '02': 2}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_datasets['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': 0, '01': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_datasets['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_resnet = models.resnet50(pretrained=True)  \n",
    "num_ftrs = mask_resnet.fc.in_features   \n",
    "mask_resnet.fc = nn.Linear(num_ftrs, 3) \n",
    "mask_resnet = mask_resnet.to(DEVICE)\n",
    " \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, mask_resnet.parameters()), lr=0.001)\n",
    " \n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) \n",
    "\n",
    "ct = 0 \n",
    "for child in mask_resnet.children():  \n",
    "    ct += 1  \n",
    "    if ct < 6: \n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_resnet = models.resnet50(pretrained=True)  \n",
    "num_ftrs = gender_resnet.fc.in_features   \n",
    "gender_resnet.fc = nn.Linear(num_ftrs, 2) \n",
    "gender_resnet = gender_resnet.to(DEVICE)\n",
    " \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, gender_resnet.parameters()), lr=0.001)\n",
    " \n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) \n",
    "\n",
    "ct = 0 \n",
    "for child in gender_resnet.children():  \n",
    "    ct += 1  \n",
    "    if ct < 6: \n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'se_resnext50_32x4d'\n",
      "=> loading checkpoint './epoch044_0.02343_3.9984.pth'\n",
      "=> loaded checkpoint './epoch044_0.02343_3.9984.pth'\n"
     ]
    }
   ],
   "source": [
    "from defaults import _C as cfg\n",
    "from model import get_model\n",
    "\n",
    "cfg.freeze()\n",
    " # create model\n",
    "print(\"=> creating model '{}'\".format(cfg.MODEL.ARCH))\n",
    "age_model = get_model(model_name=cfg.MODEL.ARCH, pretrained=None)\n",
    "\n",
    "resume_path = \"./epoch044_0.02343_3.9984.pth\"\n",
    "print(\"=> loading checkpoint '{}'\".format(resume_path))\n",
    "checkpoint = torch.load(resume_path, map_location=\"cpu\")\n",
    "age_model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}'\".format(resume_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
    "        since = time.time()                                     \n",
    "        for phase in ['train', 'test']: \n",
    "            if phase == 'train': \n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()     \n",
    " \n",
    "            running_loss = 0.0  \n",
    "            running_corrects = 0  \n",
    " \n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]: \n",
    "                inputs = inputs.to(DEVICE)  \n",
    "                labels = labels.to(DEVICE)  \n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):  \n",
    "                    outputs = model(inputs)  \n",
    "                    _, preds = torch.max(outputs, 1) \n",
    "                    loss = criterion(outputs, labels)  \n",
    "    \n",
    "                    if phase == 'train':   \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    " \n",
    "                running_loss += loss.item() * inputs.size(0)  \n",
    "                running_corrects += torch.sum(preds == labels.data)  \n",
    "            if phase == 'train':  \n",
    "                scheduler.step()\n",
    " \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
    " \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
    " \n",
    "          \n",
    "            if phase == 'test' and epoch_acc > best_acc: \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    " \n",
    "        time_elapsed = time.time() - since  \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    " \n",
    "    model.load_state_dict(best_model_wts) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 1.1246 Acc: 0.3444\n",
      "test Loss: 1.2115 Acc: 0.1623\n",
      "Completed in 1m 52s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 1.1241 Acc: 0.3430\n",
      "test Loss: 1.2133 Acc: 0.1595\n",
      "Completed in 1m 51s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 1.1241 Acc: 0.3444\n",
      "test Loss: 1.2156 Acc: 0.1598\n",
      "Completed in 1m 51s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 1.1248 Acc: 0.3438\n",
      "test Loss: 1.2099 Acc: 0.1613\n",
      "Completed in 1m 52s\n",
      "-------------- epoch 5 ----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19586/179166319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask_resnet50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_resnet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask_resnet50.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19586/873735295.py\u001b[0m in \u001b[0;36mtrain_resnet\u001b[0;34m(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mask_resnet50 = train_resnet(mask_resnet, criterion, optimizer_ft, exp_lr_scheduler, mask_dataloader, mask_dataset_sizes, num_epochs=EPOCH)\n",
    "torch.save(mask_resnet50, 'mask_resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_resnet50 = train_resnet(gender_resnet, criterion, optimizer_ft, exp_lr_scheduler, gender_dataloader, gender_dataset_sizes, num_epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
